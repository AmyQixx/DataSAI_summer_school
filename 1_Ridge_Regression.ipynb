{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to regularized regression\n",
    "\n",
    "Despite ordinary least squares (OLS) being the best unbiased linear estimator, biased (regularized) estimators can achieve lower mean-squared error especially on unseen or held out data. This is because regularization reduces variance in order to improve model predictions on cross-validated data. Regularization is also particularly useful when there is multicollinearity between your regressors. OLS suffers from large standard errors on the $\\beta$ estimates in this regime.\n",
    "\n",
    "When building encoding models for neural data, regressors often may be collinear. Let's take a basic example of a mouse that must press a lever to recieve a food reward. If a food reward is delivered every time the lever is pressed, it will be quite difficult to separate out the neural encoding of action (lever pressing) from the encoding of reward (recieving food) because these variables always occur together (and occur closely in time). When designing behavioral tasks, think how you might avoid collinearities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge, lasso and ordinary least squares (OLS)\n",
    "The goal of regression is to minimize the sum of squared errors (SSE). Below are shown the different SSE formulations for OLS, ridge, and lasso regression.\n",
    "$$\n",
    "SSE_{OLS} = \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "$$\n",
    "SSE_{Ridge} = \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2} + \\alpha\\sum_{j=1}^{P}{\\beta_j^2}\n",
    "$$\n",
    "$$\n",
    "SSE_{Lasso} = \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2} + \\lambda\\sum_{j=1}^{P}{|\\beta_j|}\n",
    "$$\n",
    "You can see that the ridge and lasso equations include an additional term that penalizes large $\\beta$ values. The only difference between ridge and lasso is that ridge is squared and lasso takes the absolute value of the weights instead. By minimizing these functions, we can compute the OLS and Ridge estimators! We will skip over the derivation of the closed form solutions, but just know that they come from minimizing the above equations. \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{OLS} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "$$\n",
    "\\hat{\\beta}_{Ridge} = (X^TX + \\alpha I)^{-1}X^Ty\n",
    "$$\n",
    "Lasso regression has no closed form solution. It must be solved with other techniques such as gradient descent, so we will omit it for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m      9\u001b[0m \u001b[39m#Import the specific functions for this class\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mencoding_tools\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#Import some sklearn functions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n",
      "File \u001b[1;32mc:\\Data\\churchland\\DataSAI_summer_school\\encoding_tools\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m \u001b[39m#Specifies the function for model fitting etc.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m \u001b[39m#Specifies the loading and saving functions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvisualizations\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m \u001b[39m#Some useful visualization functions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Data\\churchland\\DataSAI_summer_school\\encoding_tools\\utils.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEstimator, TransformerMixin\n\u001b[0;32m      5\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mstandardize_x_cols\u001b[39;00m(BaseEstimator, TransformerMixin):\n\u001b[0;32m      6\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Class to standardize specified columns in the input matrix X. This class\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m    is intended to work in an analog fashion to sklearn's StandardScaler. Inherit\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m    methods from BaseEstimator and TransformerMixin to be able to use objects of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m    ------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m    '''\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#First import the standard toolboxes\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Import the specific functions for this class\n",
    "import encoding_tools\n",
    "\n",
    "#Import some sklearn functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
