{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to regularized regression\n",
    "\n",
    "Despite ordinary least squares (OLS) being the best unbiased linear estimator for a system of equations, biased (regularized) estimators can achieve lower mean-squared errors, especially on unseen or held out data. This is particularly true when the number of predictors is high or compared to the number of observations (which can often be true for large neural encoding models)! OLS also has no unique solution when there are more predictors than samples (this is referred to as an underdetermined system), therefore it will cross-validate quite poorly in this regime. \n",
    "\n",
    "Regularization will help overcome this problem because it reduces variance in order to improve model predictions on held out data. Regularization is also particularly useful when there is multicollinearity between your regressors. OLS suffers from large standard errors on the $\\beta$ estimates in this regime.\n",
    "\n",
    "\n",
    "When building encoding models for neural data, regressors often may be collinear. Let's take a basic neuroscience example of a mouse that must press a lever to recieve a food reward. If a food reward is delivered every time the lever is pressed, it will be quite difficult to separate out the neural encoding of action (lever pressing) from the encoding of reward (recieving food) because these variables always occur together (and occur closely in time). When designing behavioral tasks, think how you might avoid collinearities. How might we modify this task so we can better isolate the encoding of action and reward?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary least squares (OLS), ridge, and lasso regression\n",
    "The goal of regression is to minimize the sum of squared errors (SSE). Below are shown the different SSE formulations for OLS, ridge, and lasso regression.\n",
    "$$\n",
    "SSE_{OLS} = \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "$$\n",
    "SSE_{Ridge} = \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2} + \\alpha\\sum_{j=1}^{P}{\\beta_j^2}\n",
    "$$\n",
    "$$\n",
    "SSE_{Lasso} = \\sum_{i=1}^{n}{(y_i - \\hat{y}_i)^2} + \\lambda\\sum_{j=1}^{P}{|\\beta_j|}\n",
    "$$\n",
    "You can see that the ridge and lasso equations include an additional term that penalizes large $\\beta$ values. The only difference between ridge and lasso is that ridge is squared and lasso takes the absolute value of the weights instead. In practice, this means that lasso will have sparse solutions (less informative predictors will have $\\beta$ weights set to 0) while ridge regression will spread out variance across predictors, even if they are correlated. \n",
    "\n",
    "By minimizing the above functions, we can compute the OLS and Ridge estimators! We will skip over the derivation of the closed form solutions, but just know that they come from minimizing the above equations. \n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{OLS} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "$$\n",
    "\\hat{\\beta}_{Ridge} = (X^TX + \\alpha I)^{-1}X^Ty\n",
    "$$\n",
    "Lasso regression has no closed form solution. It must be solved with other techniques such as gradient descent, so we will omit it for now. \n",
    "\n",
    "For the rest of this notebok we are going to implement a lot of things \"by hand\" to get an intuition of how the regression is done under the hood. In the next notebook we will make use of Python libraries (mostly sklearn) to do a lot of the heavy lifting for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First import the standard toolboxes\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import a dataset that gives the average miles per gallon (MPG) for many different cars. The displacement, horsepower, cylinders, and other information about the cars is also included. We will try to see how well we can predict fuel efficiency from these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_LABELS = ['cylinders','displacement','horsepower','weight','acceleration','model_year','american','japanese']\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "dataset = dataset.dropna()\n",
    "y = dataset.mpg.values\n",
    "x = dataset[PREDICTOR_LABELS[:-2]].to_numpy()\n",
    "\n",
    "is_american = (dataset.origin == 'usa').values #dummy variable\n",
    "is_japanese = (dataset.origin == 'japan').values #dummy variable\n",
    "\n",
    "x = np.vstack((x.T, is_american, is_japanese)).T\n",
    "print(f'The shape of x is {x.shape}')\n",
    "print(f'The shape of y is {y.shape}')\n",
    "\n",
    "# Plot each of the regressors vs MPG so we have some idea how they relate\n",
    "fig,axs = plt.subplots(4,2,figsize=(6,8),dpi=100)\n",
    "for i,ax in enumerate(np.ravel(axs)):\n",
    "    ax.scatter(x[:,i],y, s=3)\n",
    "    ax.set_xlabel(PREDICTOR_LABELS[i])\n",
    "    ax.set_ylabel('Fuel Efficiency (MPG)')\n",
    "fig.tight_layout()\n",
    "\n",
    "print('\\nGiven the plots below, what do you expect the regression weights to look like for each variable?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More data preprocessing\n",
    "x = #TODO: normalize the columns of x. Why must we do this for ridge regression?\n",
    "y = #TODO: normalize the columns of y\n",
    "y = #TODO: mean center y. Why must we do this for ridge regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1: Implement OLS and Ridge regression\n",
    "Use the equations at the beginning of the notebook to compute the $\\beta$ estimates using OLS and ridge regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_solver(x,y):\n",
    "    \"\"\"\n",
    "    TODO: use the equation at the beginning of the notebook to compute the OLS solution for beta as a function of x and y. \n",
    "    Hint: the @ symbol is equivalent to np.matmul()\n",
    "    \"\"\"\n",
    "    return betas\n",
    "\n",
    "def ridge_solver(x,y,alpha):\n",
    "    \"\"\"\n",
    "    TODO: use the equation at the beginning of the notebook to compute the ridge for beta as a function of x, y, and alpha. \n",
    "    Hint: the @ symbol is equivalent to np.matmul()\n",
    "    \"\"\"\n",
    "    return betas\n",
    "\n",
    "RIDGE_ALPHA =  #TODO: What alpha value will make ridge regression equal to ordinary least squares?\n",
    "\n",
    "ols_betas = ols_solver(x,y)\n",
    "ridge_betas = ridge_solver(x,y,RIDGE_ALPHA)\n",
    "\n",
    "for label, ols_beta, ridge_beta in zip(PREDICTOR_LABELS, ols_betas, ridge_betas):\n",
    "    print(f'OLS beta weight for {label}: {ols_beta}')\n",
    "    #print(f'ridge beta weight for {label}: {ridge_beta}')\n",
    "\n",
    "assert np.array_equiv(ols_betas,ridge_betas), \"Betas not equal, change RIDGE_ALPHA so that they will become equal\"\n",
    "print('\\nHow do the beta weights match up with what you expected from the plots above?')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2: Generate model predictions and implement a scoring function\n",
    "There are many ways to assess a models performance (many of them are also built into scikit-learn, which we will use later). But for now, lets practice building our own custom scoring function. We will use the *coefficient of determination* ($R^2$), which is a useful scoring metric to evaluate many different models.\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_i(y_i - \\hat{y}_i)^2}{\\sum_i(y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "This is the proportion of the dependent variable that is predictable from the independent variables. Usually, its value ranges from 0 to 1, though values can go negative when scoring on held out (cross-validated) data.\n",
    "\n",
    "What might a negative $R^2$ mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    TODO: using the above equation, implement a function that returns \n",
    "    the coefficient of determination (R^2) as a function of the true y\n",
    "    values and the model predictions of y (yhat in the equation above)\n",
    "    \"\"\"\n",
    "    return r_squared\n",
    "\n",
    "y_pred_ols = #TODO: generate OLS predictions using the previously calculated beta weights\n",
    "y_pred_ridge = #TODO: generate ridge predictions using the previously calculated beta weights\n",
    "\n",
    "r2_ols = r_squared_score(y,y_pred_ols)\n",
    "r2_ridge = r_squared_score(y,y_pred_ridge) # if the ridge parameter (alpha) was set to zero, these scores should be equivalent\n",
    "\n",
    "print(f'OLS R-squared: {r2_ols}')\n",
    "print(f'Ridge R-squared: {r2_ridge}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's vary the ridge parameter and see how our model performs. Do you think ridge or OLS will perform better? Where will they be equivalent? Remember, we are not doing cross-validation yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHAS = 10**np.linspace(-15,7, 100)\n",
    "\n",
    "scores = np.empty_like(ALPHAS) # empty array the same shape as ALPHAS\n",
    "\n",
    "\"\"\"\n",
    "TODO: Compute the R^2 for each value in ALPHAS and add that to the numpy array called \"scores\".\n",
    "Hint: a for loop may be helpful or you can use map() to map a function.\n",
    "\"\"\"\n",
    "\n",
    "ols_score = #TODO: compute the ols R^2 for reference\n",
    "\n",
    "# plotting\n",
    "plt.scatter(ALPHAS,scores, s=10, color='black', label='ridge')\n",
    "plt.scatter(ALPHAS[0], ols_score, s=5, color='red',label='ordinary least squares') # plot OLS score for reference\n",
    "plt.hlines(0,ALPHAS[0],ALPHAS[-1],linestyle='--',linewidth=.5, color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('Alpha')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('$R^2$ (training set)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 3: Implement cross-validation\n",
    "\n",
    "As you saw in the previous exercise, ridge regression will actually never perform better than ordinary least squares when assessing performance on training data. The power of regularization comes when generating predictions on held out data (cross-validation). When assessing model performance, it is best practice to do this on cross validated data. This ensures our model is not over-fitting and that it will be able to make reliable predictions on new data. In this toy example about fuel efficiency, we want a model that will give us a good prediction for a *new* car where the fuel efficiency is not availible. \n",
    "\n",
    "Here, we are going to implement k-fold cross validation. K-fold cross validation splits our data sets into k chunks. We will leave one chunk of the data out for testing (with our function `r_squared_score()`). We will train our model on the remaining data, and then go back and make predictions on the held out data.\n",
    "\n",
    "Here's a nice visualization of 5-fold crossvalidation\n",
    "\n",
    "![k-fold crossval figure](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AAwIlHM8TpAVe4l2FihNUQ.png)\n",
    "\n",
    "https://towardsdatascience.com/cross-validation-k-fold-vs-monte-carlo-e54df2fc179b\n",
    "\n",
    "SKLearn has several useful functions for cross-validation. We will make use of one of them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "N_FOLDS = 5\n",
    "ALPHAS = 10**np.linspace(-15,7, 100) #try some different alpha values\n",
    "\n",
    "def cross_val_score(x, y, n_folds, scoring_function, fitting_function, **fitting_func_kwargs):\n",
    "    \"\"\"\n",
    "    This function takes x and y and will return a cross-validated score with \"n_folds\" folds\n",
    "    It accepts an arbitrary scoring and fitting function and so it can be used with a vaiety of models\n",
    "    and scoring metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    kf = #TODO: create an sklearn.model_selection.KFold object with \"n_folds\" folds. Call it \"kf\". Can you make it randomly split up the data with the \"shuffle\" parameter? SKLearn's documentation will be helpful here.\n",
    "\n",
    "    train_scores = np.empty((n_folds))\n",
    "    test_scores = np.empty((n_folds))\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(x)): # iterate over cross-validation folds\n",
    "        x_train = #TODO: get train data for x\n",
    "        x_test = #TODO: get test data for x\n",
    "        y_train = #TODO: get train data for y\n",
    "        y_test = #TODO: get test data for y\n",
    "\n",
    "        betas = fitting_function(x=x_train,y=y_train,**fitting_func_kwargs)\n",
    "        y_train_predictions = x_train @ betas\n",
    "        y_test_predictions = x_test @ betas\n",
    "        train_scores[j] = scoring_function(y_train, y_train_predictions)\n",
    "        test_scores[j] = scoring_function(y_test, y_test_predictions)\n",
    "    ##############\n",
    "    \"\"\"\n",
    "    TODO: get the average training and testing scores over folds and return those\n",
    "    \"\"\"\n",
    "    avg_train_score = \n",
    "    avg_test_score = \n",
    "    ##############\n",
    "    return avg_train_score, avg_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "train_scores, test_scores = [], []\n",
    "\"\"\"\n",
    "TODO: Use the function we created above and iterate over ALPHAS to get the train_r2 and test_r2 for each alpha.\n",
    "Then append these values to the train_scores and test_scores lists\n",
    "\n",
    "Take careful note of the parameters for our cross_val_score() function. When called, it can accept arbitrary functions to fit and score.\n",
    "Luckily, we have already created these...\n",
    "\"\"\"\n",
    "################################\n",
    "\n",
    "ols_train_r2, ols_test_r2 = cross_val_score(x, y, N_FOLDS, r_squared_score, ols_solver) #run OLS for comparison\n",
    "\n",
    "# plotting\n",
    "plt.scatter(ALPHAS, train_scores, s=10, color='black',label='ridge train')\n",
    "plt.scatter(ALPHAS, test_scores, s=10, color='red',label='ridge test')\n",
    "plt.scatter(ALPHAS[0], ols_train_r2, s=10, color='blue',label='ols train')\n",
    "plt.scatter(ALPHAS[0], ols_test_r2, s=10, color='orange',label='ols test')\n",
    "plt.hlines(0,ALPHAS[0],ALPHAS[-1],linestyle='--',linewidth=.5, color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('alpha')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('$R^2$')\n",
    "\n",
    "print(f'OLS test performance: {ols_test_r2}')\n",
    "print(f'Best ridge test performance: {np.max(test_scores)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the above plot and consider the following:\n",
    "- Are the OLS or ridge models overfitting?\n",
    "- is ridge regression needed in this case?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4: Regression with collinear regressors and underdetermined systems\n",
    "\n",
    "In the plot above, you may have noticed that ridge regression test performance is only *slightly* better than ordinary least squares. This is because we have a well conditioned dataset. As we introduced at the beginning of the notebook, where regularization becomes most helpful is when you have few observations relative to the number of regressors or when regressors are collinear. Let's manipulate our regressor matrix $X$ and compare OLS and ridge again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the matrix x with noisy versions of itself\n",
    "random_offset = x + np.random.normal(scale=1e-5, size=x.shape) # same as x but with small random differences\n",
    "random_offset_2 = x + np.random.normal(scale=1e-3, size=x.shape)\n",
    "random_offset_3 = x + np.random.normal(scale=1e-7, size=x.shape)\n",
    "x_augmented = np.hstack((x, random_offset, random_offset_2, random_offset_3)) # add columns to x that are nearly identical. This is an \"ill-conditioned\" matrix but still full rank. \n",
    "\n",
    "#x_augmented = np.hstack((x, x)) # This matrix exhibits multicollinearity (it is not full rank). Uncomment and see how OLS vs ridge handle this.\n",
    "x_augmented = x_augmented / np.max(x_augmented, axis=0) #renormalize\n",
    "\n",
    "print(f'Shape of x is originally {x.shape}')\n",
    "print(f'Shape of x with data augmentation is {x_augmented.shape}')\n",
    "\n",
    "# now let's subsample x and y so that the number of observations is closer to the number of regressors\n",
    "N_SUBSAMPLE = 50 #The number of observations to randomly grab \n",
    "##################\n",
    "# TODO: Randomly subsample from x_augmented and y, grabbing \"N_SUBSAMPLE\" observations\n",
    "# Hint: np.random.choice() may be helpful\n",
    "x_subsample =\n",
    "y_subsample =\n",
    "print(f'Shape of x after subsampling is {x_subsample.shape}')\n",
    "##################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit this new dataset (x_subsample and y_subsample)\n",
    "\n",
    "################################\n",
    "ALPHAS = 10**np.linspace(-15,7, 100)\n",
    "train_scores, test_scores = [], []\n",
    "\"\"\"\n",
    "TODO: Iterate over ALPHAS to get the train_r2 and test_r2 for each alpha.\n",
    "Then append these values to the train_scores and test_scores lists. You should\n",
    "be able to copy and paste your answer from earlier.\n",
    "\"\"\"\n",
    "################################\n",
    "\n",
    "ols_train_r2, ols_test_r2 = cross_val_score(x_subsample, y_subsample, N_FOLDS, r_squared_score, ols_solver) #run OLS for comparison\n",
    "\n",
    "# plotting\n",
    "plt.scatter(ALPHAS, train_scores, s=10, color='black',label='ridge train')\n",
    "plt.scatter(ALPHAS, test_scores, s=10, color='red',label='ridge test')\n",
    "plt.scatter(ALPHAS[0], ols_train_r2, s=10, color='blue',label='ols train')\n",
    "plt.scatter(ALPHAS[0], ols_test_r2, s=10, color='orange',label='ols test')\n",
    "plt.hlines(0,ALPHAS[0],ALPHAS[-1],linestyle='--',linewidth=.5, color='black')\n",
    "plt.legend()\n",
    "plt.xlabel('alpha')\n",
    "plt.xscale('log')\n",
    "plt.ylim(-2,1.2)\n",
    "plt.ylabel('$R^2$')\n",
    "\n",
    "print(f'OLS test R^2: {ols_test_r2}')\n",
    "print(f'Best ridge test R^2: {np.max(test_scores)}')\n",
    "\n",
    "best_ridge_alpha_value = #TODO: determine what alpha value led to the highest cross validated score\n",
    "\n",
    "print(f'Best ridge parameter (alpha): {best_ridge_alpha_value}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this plot after adding in more predictor variables and reducing the number of observations and consider the following:\n",
    "- How are the two models performing? Are they overfitting?\n",
    "- Previously, the OLS and ridge fits had similar $R^2$ values, now they look wildly different for the test data. Why do you think this is?\n",
    "- See what happens when you modify `N_SUBSAMPLE`. How does this affect the results?\n",
    "- Will these methods work on a collinear matrix? Try it and see what happens\n",
    "- What will the $\\beta$ weights look like for OLS and ridge? Will there be a unique solution for OLS and ridge?\n",
    "- In your own data, how might you check to see if regressors are correlated before doing the regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up next: Selecting the proper value of alpha and fitting to neural data!\n",
    "Since $\\alpha$ is a model hyperparameter, it is important that the proper value is selected to get the best model performance. Cross-validating with many different alphas (what we did above) is one way to do that, but there are other ways it can be done. See Karabastos 2017, *Communications in Statistics* for another method of $\\alpha$ selection. The Karabastos method is the one implemented in the Musall 2019 paper we discussed.\n",
    "\n",
    "In the next notebook, we will fit an encoding model to neural data and solve it with ridge regression!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
